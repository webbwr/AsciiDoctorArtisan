name: Performance Regression Testing

on:
  push:
    branches: [main, dev, v*.*.0]
  pull_request:
    branches: [main, dev]
  workflow_dispatch:  # Allow manual runs

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for comparisons

    - name: Set up Python 3.12
      uses: actions/setup-python@v5
      with:
        python-version: '3.12'
        cache: 'pip'

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y pandoc wkhtmltopdf

    - name: Install Python dependencies
      run: |
        pip install --upgrade pip
        pip install -r requirements.txt
        pip install -e ".[dev]"

    - name: Run benchmark tests
      run: |
        pytest tests/performance/test_benchmarks.py \
          --benchmark-only \
          --benchmark-json=benchmark_results.json \
          --benchmark-autosave \
          --benchmark-save=current \
          --benchmark-verbose \
          -v

    - name: Download previous benchmark data
      continue-on-error: true
      uses: actions/cache@v4
      with:
        path: .benchmarks
        key: benchmark-${{ github.ref }}-${{ github.sha }}
        restore-keys: |
          benchmark-${{ github.ref }}-
          benchmark-refs/heads/main-

    - name: Compare benchmarks
      if: github.event_name == 'pull_request'
      run: |
        if [ -d .benchmarks ]; then
          pytest tests/performance/test_benchmarks.py \
            --benchmark-only \
            --benchmark-compare=0001 \
            --benchmark-compare-fail=max:35% \
            -v || echo "::warning::Performance regression detected (>35% slower)"
        else
          echo "No previous benchmark data found for comparison"
        fi

    - name: Store benchmark results
      uses: actions/upload-artifact@v4
      with:
        name: benchmark-results-${{ github.sha }}
        path: |
          benchmark_results.json
          .benchmarks/
        retention-days: 90

    - name: Comment PR with results
      if: github.event_name == 'pull_request' && always()
      uses: actions/github-script@v7
      with:
        script: |
          const fs = require('fs');

          if (!fs.existsSync('benchmark_results.json')) {
            console.log('No benchmark results found');
            return;
          }

          const results = JSON.parse(fs.readFileSync('benchmark_results.json', 'utf8'));
          const benchmarks = results.benchmarks;

          const fastest = benchmarks.sort((a, b) => a.stats.mean - b.stats.mean)[0];
          const slowest = benchmarks.sort((a, b) => b.stats.mean - a.stats.mean)[0];

          const comment = `## Performance Benchmark Results

          **Total Tests:** ${benchmarks.length}

          **Fastest:** \`${fastest.name}\` - ${(fastest.stats.mean * 1000).toFixed(2)} ms
          **Slowest:** \`${slowest.name}\` - ${(slowest.stats.mean * 1000).toFixed(2)} ms

          <details>
          <summary>View all benchmark results</summary>

          | Test | Mean (ms) | StdDev | Rounds |
          |------|-----------|--------|--------|
          ${benchmarks.map(b =>
            `| \`${b.name}\` | ${(b.stats.mean * 1000).toFixed(3)} | ${(b.stats.stddev * 1000).toFixed(3)} | ${b.stats.rounds} |`
          ).join('\n')}

          </details>

          [Download full results](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
          `;

          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: comment
          });

    - name: Performance regression summary
      if: always()
      run: |
        echo "## Performance Regression Testing Complete"
        echo ""
        echo "Benchmark results saved to artifacts"
        echo "Threshold: 35% regression allowed"
        echo ""
        if [ -f benchmark_results.json ]; then
          echo "Results file generated successfully"
        else
          echo "::error::No benchmark results file found"
          exit 1
        fi
